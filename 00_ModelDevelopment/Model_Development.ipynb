{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5fb969b4",
   "metadata": {},
   "source": [
    "# 0. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "db115d9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip freeze > requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7eb6889",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94c410b0",
   "metadata": {},
   "source": [
    "# 1. Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3da4cf00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "# 1. Preparar dados\n",
    "# -------------------------\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "if os.path.exists(\"./00_TrainingData\"):\n",
    "    trainset = torchvision.datasets.CIFAR10(root='./00_TrainingData', train=True,\n",
    "                                            download=False, transform=transform)\n",
    "    trainloader = torch.utils.data.DataLoader(trainset, batch_size=256,\n",
    "                                            shuffle=True)\n",
    "\n",
    "    testset = torchvision.datasets.CIFAR10(root='./00_TrainingData', train=False,\n",
    "                                        download=False, transform=transform)\n",
    "    testloader = torch.utils.data.DataLoader(testset, batch_size=256,\n",
    "                                            shuffle=False)\n",
    "else:\n",
    "    trainset = torchvision.datasets.CIFAR10(root='./00_TrainingData', train=True,\n",
    "                                            download=True, transform=transform)\n",
    "    trainloader = torch.utils.data.DataLoader(trainset, batch_size=256,\n",
    "                                            shuffle=True)\n",
    "\n",
    "    testset = torchvision.datasets.CIFAR10(root='./00_TrainingData', train=False,\n",
    "                                        download=True, transform=transform)\n",
    "    testloader = torch.utils.data.DataLoader(testset, batch_size=256,\n",
    "                                            shuffle=False)\n",
    "\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat',\n",
    "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3de8f994",
   "metadata": {},
   "source": [
    "# 2. CNN Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ab316ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "# 2. Definir CNN\n",
    "# -------------------------\n",
    "class AnimalCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AnimalCNN, self).__init__()\n",
    "        \n",
    "        # Bloco 1\n",
    "        self.conv1 = nn.Conv2d(3, 64, 3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.conv2 = nn.Conv2d(64, 64, 3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.dropout_conv = nn.Dropout(0.25)\n",
    "        \n",
    "        # Bloco 2\n",
    "        self.conv3 = nn.Conv2d(64, 128, 3, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(128)\n",
    "        self.conv4 = nn.Conv2d(128, 128, 3, padding=1)\n",
    "        self.bn4 = nn.BatchNorm2d(128)\n",
    "        self.dropout_conv2 = nn.Dropout(0.25)\n",
    "        \n",
    "        # Fully connected\n",
    "        self.fc1 = nn.Linear(128 * 8 * 8, 512)\n",
    "        self.dropout_fc = nn.Dropout(0.5)\n",
    "        self.fc2 = nn.Linear(512, 10)  # 10 classes do CIFAR-10\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Bloco 1\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = self.pool(x)\n",
    "        x = self.dropout_conv(x)\n",
    "        \n",
    "        # Bloco 2\n",
    "        x = F.relu(self.bn3(self.conv3(x)))\n",
    "        x = F.relu(self.bn4(self.conv4(x)))\n",
    "        x = self.pool(x)\n",
    "        x = self.dropout_conv2(x)\n",
    "        \n",
    "        # Flatten\n",
    "        x = x.view(-1, 128 * 8 * 8)\n",
    "        \n",
    "        # Fully connected\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout_fc(x)\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "# -------------------------\n",
    "# 3. Treinar modelo\n",
    "# -------------------------\n",
    "\n",
    "net = AnimalCNN().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68b753e9",
   "metadata": {},
   "source": [
    "# 3. CNN Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "efe47d07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50 | Loss: 1.8998 | Train Acc: 33.78% | Test Acc: 46.84%\n",
      "Epoch 2/50 | Loss: 1.3290 | Train Acc: 51.72% | Test Acc: 58.52%\n",
      "Epoch 3/50 | Loss: 1.1228 | Train Acc: 60.07% | Test Acc: 66.48%\n",
      "Epoch 4/50 | Loss: 0.9958 | Train Acc: 64.67% | Test Acc: 67.39%\n",
      "Epoch 5/50 | Loss: 0.9107 | Train Acc: 68.10% | Test Acc: 73.50%\n",
      "Epoch 6/50 | Loss: 0.8410 | Train Acc: 70.71% | Test Acc: 74.80%\n",
      "Epoch 7/50 | Loss: 0.7887 | Train Acc: 72.59% | Test Acc: 75.76%\n",
      "Epoch 8/50 | Loss: 0.7507 | Train Acc: 73.74% | Test Acc: 76.22%\n",
      "Epoch 9/50 | Loss: 0.7167 | Train Acc: 74.98% | Test Acc: 78.07%\n",
      "Epoch 10/50 | Loss: 0.6760 | Train Acc: 76.39% | Test Acc: 78.55%\n",
      "Epoch 11/50 | Loss: 0.6402 | Train Acc: 77.66% | Test Acc: 78.95%\n",
      "Epoch 12/50 | Loss: 0.6131 | Train Acc: 78.76% | Test Acc: 80.27%\n",
      "Epoch 13/50 | Loss: 0.5909 | Train Acc: 79.57% | Test Acc: 78.73%\n",
      "Epoch 14/50 | Loss: 0.5701 | Train Acc: 80.09% | Test Acc: 79.39%\n",
      "Epoch 15/50 | Loss: 0.5404 | Train Acc: 81.09% | Test Acc: 79.41%\n",
      "Epoch 16/50 | Loss: 0.5256 | Train Acc: 81.60% | Test Acc: 80.13%\n",
      "Epoch 17/50 | Loss: 0.5050 | Train Acc: 82.17% | Test Acc: 80.46%\n",
      "Epoch 18/50 | Loss: 0.4881 | Train Acc: 82.90% | Test Acc: 80.81%\n",
      "Epoch 19/50 | Loss: 0.4658 | Train Acc: 83.54% | Test Acc: 82.76%\n",
      "Epoch 20/50 | Loss: 0.4515 | Train Acc: 84.15% | Test Acc: 81.76%\n",
      "Epoch 21/50 | Loss: 0.3876 | Train Acc: 86.34% | Test Acc: 83.61%\n",
      "Epoch 22/50 | Loss: 0.3689 | Train Acc: 86.93% | Test Acc: 83.68%\n",
      "Epoch 23/50 | Loss: 0.3568 | Train Acc: 87.38% | Test Acc: 83.30%\n",
      "Epoch 24/50 | Loss: 0.3437 | Train Acc: 87.73% | Test Acc: 83.91%\n",
      "Epoch 25/50 | Loss: 0.3352 | Train Acc: 88.02% | Test Acc: 84.56%\n",
      "Epoch 26/50 | Loss: 0.3284 | Train Acc: 88.10% | Test Acc: 84.34%\n",
      "Epoch 27/50 | Loss: 0.3262 | Train Acc: 88.23% | Test Acc: 83.96%\n",
      "Epoch 28/50 | Loss: 0.3105 | Train Acc: 88.86% | Test Acc: 84.06%\n",
      "Epoch 29/50 | Loss: 0.3061 | Train Acc: 88.98% | Test Acc: 84.18%\n",
      "Epoch 30/50 | Loss: 0.3031 | Train Acc: 88.99% | Test Acc: 84.40%\n",
      "Epoch 31/50 | Loss: 0.2949 | Train Acc: 89.55% | Test Acc: 84.06%\n",
      "Epoch 32/50 | Loss: 0.2912 | Train Acc: 89.61% | Test Acc: 84.38%\n",
      "Epoch 33/50 | Loss: 0.2790 | Train Acc: 89.98% | Test Acc: 84.20%\n",
      "Epoch 34/50 | Loss: 0.2768 | Train Acc: 90.02% | Test Acc: 84.66%\n",
      "Epoch 35/50 | Loss: 0.2654 | Train Acc: 90.40% | Test Acc: 84.60%\n",
      "Epoch 36/50 | Loss: 0.2586 | Train Acc: 90.60% | Test Acc: 84.45%\n",
      "Epoch 37/50 | Loss: 0.2547 | Train Acc: 90.82% | Test Acc: 84.76%\n",
      "Epoch 38/50 | Loss: 0.2484 | Train Acc: 91.05% | Test Acc: 84.91%\n",
      "Epoch 39/50 | Loss: 0.2493 | Train Acc: 90.99% | Test Acc: 84.73%\n",
      "Epoch 40/50 | Loss: 0.2400 | Train Acc: 91.31% | Test Acc: 83.99%\n",
      "Epoch 41/50 | Loss: 0.2194 | Train Acc: 91.91% | Test Acc: 84.97%\n",
      "Epoch 42/50 | Loss: 0.2057 | Train Acc: 92.49% | Test Acc: 85.21%\n",
      "Epoch 43/50 | Loss: 0.2022 | Train Acc: 92.66% | Test Acc: 85.16%\n",
      "Epoch 44/50 | Loss: 0.1975 | Train Acc: 92.79% | Test Acc: 85.33%\n",
      "Epoch 45/50 | Loss: 0.1961 | Train Acc: 92.90% | Test Acc: 85.43%\n",
      "Epoch 46/50 | Loss: 0.1927 | Train Acc: 93.06% | Test Acc: 85.04%\n",
      "Epoch 47/50 | Loss: 0.1937 | Train Acc: 93.06% | Test Acc: 85.14%\n",
      "Epoch 48/50 | Loss: 0.1850 | Train Acc: 93.37% | Test Acc: 85.32%\n",
      "Epoch 49/50 | Loss: 0.1853 | Train Acc: 93.37% | Test Acc: 85.49%\n",
      "Epoch 50/50 | Loss: 0.1814 | Train Acc: 93.40% | Test Acc: 85.25%\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.5)\n",
    "\n",
    "# -------------------------\n",
    "# 4. Treinamento\n",
    "# -------------------------\n",
    "num_epochs = 50\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    net.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for inputs, labels in trainloader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    scheduler.step()\n",
    "    train_loss = running_loss / len(trainloader.dataset)\n",
    "    train_acc = 100. * correct / total\n",
    "    \n",
    "    # Avaliação no teste\n",
    "    net.eval()\n",
    "    correct_test = 0\n",
    "    total_test = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in testloader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = net(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total_test += labels.size(0)\n",
    "            correct_test += (predicted == labels).sum().item()\n",
    "    test_acc = 100. * correct_test / total_test\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} | Loss: {train_loss:.4f} | Train Acc: {train_acc:.2f}% | Test Acc: {test_acc:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1a2df47",
   "metadata": {},
   "source": [
    "# 4. Testing with new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "012db4ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "import torch\n",
    "\n",
    "# Transformações iguais às usadas no treino\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((32, 32)),  # CIFAR-10 tem 32x32\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "# Carregar a imagem\n",
    "img = Image.open(\"./01_TestingDataFromInternet/airplane.jpg\").convert(\"RGB\")\n",
    "\n",
    "# Aplicar as transformações\n",
    "img_tensor = transform(img)\n",
    "\n",
    "# Adicionar dimensão de batch [1, 3, 32, 32]\n",
    "img_tensor = img_tensor.unsqueeze(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "05e49a50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classe prevista: plane\n"
     ]
    }
   ],
   "source": [
    "net.eval()  # modo de avaliação\n",
    "with torch.no_grad():\n",
    "    img_tensor = img_tensor.to(device)\n",
    "    outputs = net(img_tensor)\n",
    "    _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "print(\"Classe prevista:\", classes[predicted.item()])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "537f06e8",
   "metadata": {},
   "source": [
    "# 5. Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f3e04c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(net.state_dict(), 'CNN_Model.pth')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML_Project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
